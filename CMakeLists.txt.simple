cmake_minimum_required(VERSION 3.20)
project(shepherd VERSION 1.0.0 LANGUAGES CXX)

set(CMAKE_CXX_STANDARD 17)
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# Skip llama.cpp - use TensorRT or API backends only
find_package(PkgConfig REQUIRED)
message(STATUS "Configured for TensorRT/API backends (llama.cpp disabled)")

# Main executable
add_executable(shepherd
    main.cpp
    inference_engine.cpp
    rag_system.cpp
    memory_manager.cpp
)

target_include_directories(shepherd PRIVATE ${INFERENCE_INCLUDES})
target_link_libraries(shepherd PRIVATE ${INFERENCE_LIBS})

# Compiler-specific options
if(CMAKE_CXX_COMPILER_ID STREQUAL "GNU" OR CMAKE_CXX_COMPILER_ID STREQUAL "Clang")
    target_compile_options(shepherd PRIVATE -Wall -Wextra -O3)
endif()