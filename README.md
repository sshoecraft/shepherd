# Shepherd

**Advanced Multi-Backend LLM System with Intelligent Memory Management**

Shepherd is a production-grade C++ LLM inference system supporting both local models (llama.cpp, TensorRT-LLM) and cloud APIs (OpenAI, Anthropic, Gemini, Grok, Ollama). It features sophisticated KV cache eviction policies, retrieval-augmented generation (RAG), hierarchical memory management, and comprehensive tool/function calling capabilities.

---

## Key Features

### ðŸ”„ Multi-Backend Architecture
- **8 Backend Implementations**: Seamlessly switch between local inference and cloud APIs
  - **Local**: llama.cpp (CPU/GPU), TensorRT-LLM (NVIDIA optimized)
  - **Cloud**: OpenAI, Anthropic Claude, Google Gemini, xAI Grok, Ollama
- **Unified Interface**: Single API across all backends with automatic fallback
- **Dynamic Model Selection**: Runtime backend selection based on model availability

### ðŸ§  Intelligent KV Cache Eviction
- **Callback-Driven Eviction** (llama.cpp): Interrupt-based eviction triggered by memory pressure
- **Event-Driven Eviction** (TensorRT): Asynchronous monitoring with `KVCacheEventManager`
- **Smart Message Preservation**: Protects system prompts and current context
- **Automatic Archival**: Evicted messages stored in RAG database for retrieval
- **Position Shift Management**: Maintains KV cache consistency during eviction

### ðŸ“š RAG System (Retrieval-Augmented Generation)
- **Memory-Mapped SQLite Storage**: Persistent conversation memory with mmap I/O optimization
- **FTS5 Full-Text Search**: Fast semantic search with BM25 ranking algorithm
- **Fact Storage**: Persistent key-value store for long-term knowledge
- **Automatic Archival**: Seamless integration with eviction system
- **Configurable Retention**: Database size limits and auto-pruning (default 10GB)

### ðŸ› ï¸ Comprehensive Tools System
- **10+ Built-in Tools** across 6 categories:
  - **Filesystem**: read_file, write_file, list_directory, delete_file, file_exists
  - **Command Execution**: execute_command with timeout and signal handling
  - **HTTP Client**: GET, POST, PUT, DELETE with custom headers
  - **JSON Processing**: parse, validate, pretty-print, JSONPath extraction
  - **Memory Management**: search_memory, set_fact, get_fact, store_memory
  - **MCP Integration**: list_resources, read_resource, call_mcp_tool
- **Schema-Driven**: Structured parameter definitions with JSON schema support
- **Extensible**: Easy registration of custom tools via ToolRegistry

### ðŸ”Œ Model Context Protocol (MCP)
- **Full MCP Client/Server**: JSON-RPC 2.0 over stdio
- **External Tool Delegation**: Seamlessly integrate external MCP servers
- **Resource Access**: Read resources from MCP servers (files, databases, APIs)
- **Prompt Templates**: Reusable prompt templates from MCP servers

### ðŸ’¾ Hierarchical Memory Management
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU VRAM      â”‚    â”‚   System RAM     â”‚    â”‚    Storage      â”‚
â”‚   (Tier 1)      â”‚ âŸ¶  â”‚   (Tier 2)       â”‚ âŸ¶  â”‚   (Tier 3)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ KV Cache      â”‚    â”‚ â€¢ Message Buffer â”‚    â”‚ â€¢ SQLite DB     â”‚
â”‚ â€¢ Active Tokens â”‚    â”‚ â€¢ Token Pools    â”‚    â”‚ â€¢ Conversation  â”‚
â”‚ â€¢ Model Weights â”‚    â”‚ â€¢ Spill Cache    â”‚    â”‚ â€¢ Vector Store  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Hot (~ms)              Warm (~Î¼s)            Cold (~10ms)
```

---

## Supported Backends

### Local Inference (Stateful)

#### **llama.cpp**
- **GPU Support**: CUDA with automatic layer offloading
- **Model Families**: Llama 3.x, Qwen 2.x, GLM-4, Mistral, Gemma, Phi, DeepSeek
- **Chat Templates**: Jinja2-based template rendering
- **Context Windows**: 8K - 256K+ tokens (model dependent)
- **KV Cache**: Callback-driven eviction with position tracking
- **Tokenization**: Accurate token counting via model vocabulary

#### **TensorRT-LLM**
- **Optimization**: NVIDIA TensorRT for maximum GPU performance
- **Multi-GPU**: MPI-based layer distribution across GPUs
- **Plugin Architecture**: Extensible with custom kernels
- **Context Windows**: 2K - 256K+ tokens
- **KV Cache**: Event-driven monitoring with asynchronous eviction
- **Production Ready**: Enterprise-grade inference with monitoring

### Cloud APIs (Stateless)

| Backend | Models | Context | Tools | Notes |
|---------|--------|---------|-------|-------|
| **OpenAI** | GPT-4, GPT-3.5-Turbo | 128K-200K | âœ“ | Function calling |
| **Anthropic** | Claude 3/3.5 (Opus, Sonnet, Haiku) | 200K | âœ“ | Separate system field |
| **Gemini** | Gemini Pro, 1.5 Pro/Flash, 2.0 | 32K-2M | âœ“ | SentencePiece tokens |
| **Grok** | Grok-1, Grok-2 | 128K | âœ“ | OpenAI-compatible |
| **Ollama** | Any Ollama model | 8K-128K | âœ“ | Local/containerized |

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                        â”‚
â”‚  â€¢ main.cpp: Interactive loop, tool execution               â”‚
â”‚  â€¢ HTTP Server: REST API for external integrations          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend Manager                           â”‚
â”‚  â€¢ Unified interface across all backends                    â”‚
â”‚  â€¢ Automatic backend selection and fallback                 â”‚
â”‚  â€¢ Message formatting and tool conversion                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LlamaCpp   â”‚  â”‚ TensorRT   â”‚  â”‚ API Backendsâ”‚
â”‚ Backend    â”‚  â”‚ Backend    â”‚  â”‚ (5 types)   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Context Manager (Base)             â”‚
â”‚  â€¢ Message storage and token tracking        â”‚
â”‚  â€¢ Eviction calculation and archival         â”‚
â”‚  â€¢ Context utilization monitoring            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”  â”Œâ”€â”€â”€â”€vâ”€â”€â”€â”€â”  â”Œâ”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG      â”‚  â”‚ Tools   â”‚  â”‚ MCP Client   â”‚
â”‚ System   â”‚  â”‚ System  â”‚  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## KV Cache Eviction System

### Why Eviction Matters

Modern LLMs have limited GPU memory for storing conversation context (KV cache). A 70B model with 128K context window requires ~240GB of KV cache memory. Shepherd's eviction system enables **indefinite conversations** by intelligently managing this constraint.

### Eviction Strategies

#### **llama.cpp: Callback-Driven**
```cpp
// Triggered by llama.cpp when KV cache fills
ctx_params.kv_need_space_callback = [](uint32_t tokens_needed, void* user_data) {
    return backend->evict_to_free_space(tokens_needed);
};
```

**Process**:
1. Calculate oldest evictable messages (preserving system prompt and current message)
2. Map message indices to KV cache token positions
3. Remove token range from KV cache: `llama_memory_seq_rm()`
4. Shift remaining tokens to eliminate gaps: `llama_memory_seq_add()`
5. Archive evicted messages to RAG database
6. Insert eviction notice into context
7. Return new KV cache head position

#### **TensorRT: Event-Driven**
```cpp
// Background thread monitors KV cache events
void monitor_kv_events() {
    auto events = event_mgr->getLatestEvents(100ms);
    for (auto& event : events) {
        if (event.type == KVCacheRemovedData) {
            handle_eviction(event.block_hashes);
        }
    }
}
```

**Process**:
1. Asynchronous monitoring thread detects `KVCacheRemovedData` events
2. Estimate tokens removed (~64-128 tokens per block)
3. Calculate corresponding message range
4. Archive messages to RAG database
5. Update context state

### Protection Rules
- **System Message**: Never evicted (contains tools and instructions)
- **Current User Message**: Never evicted (needed for generation)
- **Recent Tool Calls**: Protected until response generated
- **Eviction Order**: Oldest-first (LRU) within evictable range

---

## Building Shepherd

### Prerequisites

**Required**:
- C++17 compiler (GCC 9+, Clang 10+, MSVC 2019+)
- CMake 3.18+
- SQLite3
- libcurl
- nlohmann/json

**Optional** (for local inference):
- llama.cpp (for CPU/GPU inference)
- TensorRT-LLM (for NVIDIA GPU optimization)
- CUDA Toolkit 11.8+ (for GPU support)

### Build Instructions

```bash
# Clone repository
git clone https://github.com/sshoecraft/shepherd.git
cd shepherd

# Build with CMake
mkdir build && cd build
cmake ..
make -j$(nproc)

# Run
./shepherd
```

### CMake Options

```bash
# Build with llama.cpp support
cmake -DUSE_LLAMACPP=ON ..

# Build with TensorRT-LLM support
cmake -DUSE_TENSORRT=ON ..

# Build with all backends (default)
cmake -DUSE_LLAMACPP=ON -DUSE_TENSORRT=ON ..

# Debug build
cmake -DCMAKE_BUILD_TYPE=Debug ..
```

---

## Configuration

### Backend Selection

Shepherd automatically detects available backends at runtime. Priority order:
1. **TensorRT-LLM** (if compiled and NVIDIA GPU detected)
2. **llama.cpp** (if compiled)
3. **API backends** (if API keys configured)

### Environment Variables

```bash
# API Keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GOOGLE_API_KEY="..."
export GROK_API_KEY="xai-..."

# Local Model Path (for llama.cpp/TensorRT)
export MODEL_PATH="/path/to/model.gguf"

# RAG Database
export RAG_DB_PATH="./shepherd_memory.db"
export RAG_MAX_SIZE="10737418240"  # 10GB

# Backend Override
export SHEPHERD_BACKEND="llamacpp"  # or "tensorrt", "openai", etc.
```

### Configuration File (shepherd.json)

```json
{
    "backend": "llamacpp",
    "model_path": "/models/llama-3.1-70b-instruct.gguf",
    "context_size": 131072,
    "gpu_layers": 48,
    "api_keys": {
        "openai": "sk-...",
        "anthropic": "sk-ant-..."
    },
    "rag": {
        "database_path": "./shepherd_memory.db",
        "max_size_bytes": 10737418240,
        "enable_archival": true
    },
    "tools": {
        "enabled": ["filesystem", "http", "memory", "command"],
        "disabled": ["command_execution"]
    },
    "mcp_servers": [
        {
            "name": "filesystem",
            "command": "npx",
            "args": ["-y", "@modelcontextprotocol/server-filesystem", "/home/user"]
        }
    ]
}
```

---

## Usage Examples

### Interactive Mode

```bash
$ ./shepherd /models/llama-3.1-70b-instruct.gguf

Shepherd LLM System v1.0
Backend: llama.cpp (CUDA)
Model: llama-3.1-70b-instruct
Context: 131072 tokens
Tools: 12 available

> What files are in the current directory?
[Tool Call: list_directory(path=".")]
[Tool Result: main.cpp, README.md, CMakeLists.txt, ...]

The current directory contains:
- main.cpp: Main application entry point
- README.md: Project documentation
- CMakeLists.txt: Build configuration
...

> Read the contents of README.md and summarize it
[Tool Call: read_file(path="README.md")]
[Tool Result: # Shepherd\n\nAdvanced Multi-Backend LLM System...]

This README describes Shepherd, a production-grade LLM inference system...
```

### Python API

```python
import requests

# Start conversation
response = requests.post("http://localhost:8080/v1/chat/completions", json={
    "model": "llama-3.1-70b-instruct",
    "messages": [
        {"role": "user", "content": "What's the weather like?"}
    ],
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "http_get",
                "description": "Make HTTP GET request",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string"}
                    }
                }
            }
        }
    ]
})

print(response.json())
```

### Tool Execution

```bash
# Enable specific tools
./shepherd --enable-tools filesystem,http,memory

# Disable dangerous tools
./shepherd --disable-tools command_execution

# List available tools
./shepherd --list-tools
```

---

## RAG System Usage

### Automatic Archival

When messages are evicted from KV cache, they're automatically archived:

```
[User]: What's the capital of France?
[Assistant]: Paris is the capital of France.

... 100K tokens later ...

[NOTICE: 2 messages evicted from context and archived to memory]
```

### Semantic Search

Retrieve relevant past conversations:

```python
# Search archived conversations
results = rag_manager.search("capital of France", max_results=5)

for result in results:
    print(f"Relevance: {result.score}")
    print(f"User: {result.user_message}")
    print(f"Assistant: {result.assistant_response}")
```

### Fact Storage

Store persistent knowledge across sessions:

```bash
> Remember that my favorite color is blue
[Tool Call: set_fact(key="favorite_color", value="blue")]
[Tool Result: {"success": true}]

I've stored that your favorite color is blue.

... new session ...

> What's my favorite color?
[Tool Call: get_fact(key="favorite_color")]
[Tool Result: {"value": "blue"}]

Your favorite color is blue!
```

---

## Hardware Recommendations

### Optimal Configuration
- **GPUs**: 2x NVIDIA RTX 3090 (48GB VRAM) or better
- **RAM**: 128GB system RAM
- **Storage**: NVMe SSD (1TB+)
- **CPU**: 16+ cores for parallel processing

### Minimum Configuration
- **GPU**: NVIDIA GTX 1080 Ti (11GB VRAM) or better
- **RAM**: 32GB system RAM
- **Storage**: SATA SSD (500GB)
- **CPU**: 8 cores

### Cloud Deployment
- **AWS**: g5.12xlarge (4x A10G, 192GB RAM)
- **GCP**: a2-highgpu-4g (4x A100, 340GB RAM)
- **Azure**: Standard_NC24ads_A100_v4 (A100 80GB)

---

## Performance Characteristics

### Throughput (70B model, batch_size=1)

| Backend | Prompt Speed | Generation Speed | Latency |
|---------|--------------|------------------|---------|
| TensorRT-LLM | 8000 tok/s | 45 tok/s | ~50ms |
| llama.cpp (CUDA) | 1200 tok/s | 25 tok/s | ~80ms |
| llama.cpp (CPU) | 150 tok/s | 8 tok/s | ~200ms |

### Memory Usage (70B model)

| Configuration | VRAM | System RAM | Context Size |
|---------------|------|------------|--------------|
| Q4_K_M + 64K ctx | 38GB | 8GB | 65536 tokens |
| Q4_K_M + 128K ctx | 42GB | 12GB | 131072 tokens |
| Q8_0 + 64K ctx | 72GB | 16GB | 65536 tokens |

### Eviction Performance

- **Eviction latency**: <10ms (llama.cpp), <5ms (TensorRT)
- **Archive write**: ~2ms per conversation turn (SQLite)
- **Search latency**: ~15ms (FTS5 full-text search)
- **Context shift**: O(n) where n = remaining tokens

---

## Development

### Project Structure

```
shepherd/
â”œâ”€â”€ backends/           # Backend implementations
â”‚   â”œâ”€â”€ api_backend.{cpp,h}      # Base for API backends
â”‚   â”œâ”€â”€ llamacpp.{cpp,h}         # llama.cpp backend
â”‚   â”œâ”€â”€ tensorrt.{cpp,h}         # TensorRT-LLM backend
â”‚   â”œâ”€â”€ openai.{cpp,h}           # OpenAI API
â”‚   â”œâ”€â”€ anthropic.{cpp,h}        # Anthropic Claude
â”‚   â”œâ”€â”€ gemini.{cpp,h}           # Google Gemini
â”‚   â”œâ”€â”€ grok.{cpp,h}             # xAI Grok
â”‚   â””â”€â”€ ollama.{cpp,h}           # Ollama
â”œâ”€â”€ tools/              # Tool implementations
â”‚   â”œâ”€â”€ tool.{cpp,h}             # Base tool interface
â”‚   â”œâ”€â”€ filesystem_tools.{cpp,h}
â”‚   â”œâ”€â”€ http_tools.{cpp,h}
â”‚   â”œâ”€â”€ command_tools.{cpp,h}
â”‚   â”œâ”€â”€ json_tools.{cpp,h}
â”‚   â””â”€â”€ memory_tools.{cpp,h}
â”œâ”€â”€ mcp/                # Model Context Protocol
â”‚   â”œâ”€â”€ mcp_client.{cpp,h}
â”‚   â”œâ”€â”€ mcp_server.{cpp,h}
â”‚   â””â”€â”€ mcp_manager.{cpp,h}
â”œâ”€â”€ rag_system.{cpp,h}  # RAG implementation
â”œâ”€â”€ memory_manager.{cpp,h}       # Memory management
â”œâ”€â”€ context_manager.{cpp,h}      # Context handling
â”œâ”€â”€ backend_manager.{cpp,h}      # Backend orchestration
â”œâ”€â”€ tokenizer.{cpp,h}            # Tokenization
â”œâ”€â”€ main.cpp            # Application entry
â””â”€â”€ CMakeLists.txt      # Build configuration
```

### Adding Custom Tools

```cpp
#include "tools/tool.h"

class MyCustomTool : public Tool {
public:
    std::string unsanitized_name() const override {
        return "my_custom_tool";
    }

    std::string description() const override {
        return "Does something amazing";
    }

    std::string parameters() const override {
        return R"({
            "type": "object",
            "properties": {
                "input": {"type": "string", "description": "Input text"}
            },
            "required": ["input"]
        })";
    }

    std::map<std::string, std::any> execute(
        const std::map<std::string, std::any>& args) override
    {
        std::string input = std::any_cast<std::string>(args.at("input"));
        // Do something with input
        return {{"result", "success"}};
    }
};

// Register tool
ToolRegistry::instance().register_tool(std::make_unique<MyCustomTool>());
```

### Adding Custom Backends

```cpp
#include "backend_interface.h"

class MyBackend : public Backend {
public:
    bool initialize(const std::string& model_path) override {
        // Initialize your backend
        return true;
    }

    std::string generate(const std::string& prompt,
                        const GenerationParams& params) override {
        // Generate response
        return response;
    }

    void add_message(const Message& message) override {
        // Add to context
    }

    // Implement other virtual methods...
};

// Register backend
BackendManager::register_backend("mybackend",
    std::make_unique<MyBackend>());
```

---

## Troubleshooting

### Out of Memory During Inference

**Solution**: Reduce context size or enable eviction
```bash
./shepherd --context-size 65536 --enable-eviction
```

### Slow Generation Speed

**Solution**: Increase GPU layers or use TensorRT
```bash
./shepherd --gpu-layers 48 --backend tensorrt
```

### Tool Execution Failures

**Solution**: Check tool permissions and enable verbose logging
```bash
./shepherd --verbose --log-level debug
```

### KV Cache Corruption

**Symptoms**: Repetitive or nonsensical output after eviction

**Solution**: Verify position shifting is working correctly. Check logs for:
```
[DEBUG] Evicting messages 2-5 (tokens 1024-4096)
[DEBUG] Shifting remaining tokens by -3072
[DEBUG] New KV cache size: 8192 tokens
```

---

## Documentation

- **API Reference**: [docs/api.md](docs/api.md)
- **Architecture Guide**: [docs/architecture.md](docs/architecture.md)
- **Tool Development**: [docs/tools.md](docs/tools.md)
- **Backend Development**: [docs/backends.md](docs/backends.md)
- **MCP Integration**: [docs/mcp.md](docs/mcp.md)

---

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Areas for Contribution
- Additional backend integrations
- New tool implementations
- Performance optimizations
- Documentation improvements
- Test coverage

---

## License

[Specify License]

---

## Acknowledgments

- **llama.cpp**: Georgi Gerganov and contributors
- **TensorRT-LLM**: NVIDIA Corporation
- **Model Context Protocol**: Anthropic
- **SQLite**: D. Richard Hipp

---

## Contact

- **Issues**: https://github.com/sshoecraft/shepherd/issues
- **Discussions**: https://github.com/sshoecraft/shepherd/discussions
