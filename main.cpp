
#include "shepherd.h"
#include "tools/tools.h"
#include "backends/llamacpp.h"  // Include before mcp.h to define json as ordered_json
#include "mcp/mcp.h"
#include "rag.h"
#include "server/server.h"
#include "cli.h"
#include "backends/backend.h"
#include "backends/factory.h"

#include <iostream>
#include <fstream>
#include <string>
#include <cstring>
#include <getopt.h>
#include <csignal>
#include <memory>
#include <unistd.h>
#include <filesystem>
#include <termios.h>
#include <sys/select.h>
#include <fcntl.h>
#include <atomic>
#include <thread>
#include <sys/socket.h>
#include <sys/wait.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <cerrno>

// Global storage for original command-line arguments (for MPI re-exec)
static int g_argc = 0;
static char** g_argv = nullptr;

void set_global_args(int argc, char** argv) {
	g_argc = argc;
	g_argv = argv;
}

void get_global_args(int& argc, char**& argv) {
	argc = g_argc;
	argv = g_argv;
}

#ifdef USE_READLINE
#include <readline/readline.h>
#include <readline/history.h>
#endif

// Global debug level (0=off, 1-9=increasing verbosity)
// Used by dprintf() macro in debug.h for fine-grained debug control
int g_debug_level = 0;

// Global config instance
std::unique_ptr<Config> config;

// Global server mode flag
bool g_server_mode = false;

// Global generation cancellation flag (for llamacpp backend)
bool g_generation_cancelled = false;


static void print_usage(int, char** argv) {
	printf("\n=== Shepherd - Advanced LLM Management System ===\n");
	printf("\nUsage:\n");
	printf("	%s [OPTIONS]\n", argv[0]);
	printf("	%s edit-system				Edit system prompt in $EDITOR\n", argv[0]);
	printf("	%s list-tools				List all available tools\n", argv[0]);
	printf("	%s mcp <add|remove|list> [args...]\n", argv[0]);
	printf("\nOptions:\n");
	printf("	-c, --config FILE  Specify config file (default: ~/.shepherd/config.json)\n");
	printf("	-d, --debug[=N]    Enable debug mode with optional level (1-9, default: 1)\n");
	printf("	-l, --log-file	   Log to file instead of console\n");
	printf("	-m, --model		   Model name or file (overrides config)\n");
	printf("	--model_path	   Model directory path (overrides config, e.g., ~/models)\n");
	printf("	--backend		   Backend (llamacpp, openai, anthropic, gemini, grok, ollama)\n");
	printf("	--api-key		   API key for cloud backends\n");
	printf("	--api-base		   API base URL (for OpenAI-compatible APIs)\n");
	printf("	--context-size	   Set context window size (0 = use model's full context, default: from config)\n");
	printf("	--max-tokens	   Set max generation tokens (default: auto)\n");
	printf("	--memory-db		   Path to RAG memory database (default: ~/.shepherd/memory.db)\n");
	printf("	--nomcp			   Disable MCP system (no MCP servers loaded)\n");
	printf("	--template		   Custom chat template file (Jinja format, llamacpp only)\n");
	printf("	--server		   Start HTTP API server mode (OpenAI-compatible)\n");
	printf("	--port PORT		   Server port (default: 8000, requires --server)\n");
	printf("	--host HOST		   Server host to bind to (default: 0.0.0.0, requires --server)\n");
	printf("	--truncate LIMIT   Truncate tool results to LIMIT tokens (0 = auto 85%% of available space)\n");
	printf("	-h, --help		   Show this help message\n");
	printf("\nMCP Management:\n");
	printf("	mcp list							  List all configured MCP servers\n");
	printf("	mcp add <name> <cmd> [args] [-e ...]  Add a new MCP server\n");
	printf("										  -e KEY=VALUE	Set environment variable\n");
	printf("	mcp remove <name>					  Remove an MCP server\n");
	printf("\nConfiguration:\n");
	printf("	Edit ~/.shepherd/config.json to configure:\n");
	printf("	- backend: llamacpp, openai, anthropic");
#ifdef PLATFORM_LINUX
#ifdef ENABLE_TENSORRT
	printf(", tensorrt");
#endif
#endif
	printf("\n");
	printf("	- model: model name or path\n");
	printf("	- model_path: directory for models (optional, defaults to ~/.shepherd/models)\n");
	printf("	- key: API key for cloud backends (optional)\n");
	printf("	- context_size: context window size (optional, 0 = auto)\n");
	printf("\nFeatures:\n");
	printf("	- Direct document reading and processing\n");
	printf("	- Conversation memory with search capabilities\n");
	printf("	- Multiple inference backends (local + cloud)\n");
	printf("	- Tool execution support\n");
	printf("	- Model Context Protocol (MCP) server integration\n");
	printf("\n");
}

static void signal_handler(int signal) {
	// Terminal cleanup is handled by CLI destructor
	printf("\n\nReceived signal %d, shutting down gracefully...\n", signal);
	exit(0);
}

static void cancel_handler(int signal) {
	(void)signal; // Suppress unused parameter warning
	// Set cancellation flag when SIGUSR1 received (from FastAPI on client disconnect)
	// g_generation_cancelled = true;  // This would need to be in CLI now
	LOG_INFO("Received SIGUSR1 - cancelling generation");
}

#if 0
static void show_config(const Config& config) {
	printf("\n=== Current Configuration ===\n");
	printf("Backend: %s\n", config.backend.c_str());
	printf("Model: %s\n", config.model.c_str());

	if (config.backend == "llamacpp" || config.backend == "tensorrt") {
		printf("Model path: %s\n", config.model_path.c_str());
	} else if (config.backend == "openai" || config.backend == "anthropic") {
		printf("API key: %s\n", config.key.empty() ? "(not set)" : "***");
	}

	printf("Context size: %zu\n", config.context_size);

	printf("\nAvailable backends: ");
	auto available = Config::get_available_backends();
	for (size_t i = 0; i < available.size(); ++i) {
		if (i > 0) printf(", ");
		printf("%s", available[i].c_str());
	}
	printf("\n\n");
}
#endif

static int handle_mcp_command(int argc, char** argv) {
	if (argc < 3) {
		std::cerr << "Usage: shepherd mcp <add|remove|list> [args...]" << std::endl;
		return 1;
	}

	std::string subcommand = argv[2];
	std::string config_path = std::string(getenv("HOME")) + "/.shepherd/config.json";

	if (subcommand == "list") {
		// Suppress logs during health check
		Logger::instance().set_log_level(LogLevel::FATAL);
		MCPConfig::list_servers(config_path, true);  // Check health
		return 0;
	}

	if (subcommand == "add") {
		if (argc < 5) {
			std::cerr << "Usage: shepherd mcp add <name> <command> [args...] [-e KEY=VALUE ...]" << std::endl;
			return 1;
		}

		MCPServerEntry server;
		server.name = argv[3];
		server.command = argv[4];

		// Parse arguments and environment variables
		for (int i = 5; i < argc; i++) {
			std::string arg = argv[i];

			if (arg == "-e" || arg == "--env") {
				// Next arg should be KEY=VALUE
				if (i + 1 < argc) {
					i++;
					std::string env_pair = argv[i];
					size_t eq_pos = env_pair.find('=');
					if (eq_pos != std::string::npos) {
						std::string key = env_pair.substr(0, eq_pos);
						std::string value = env_pair.substr(eq_pos + 1);
						server.env[key] = value;
					} else {
						std::cerr << "Warning: Invalid env format (use KEY=VALUE): " << env_pair << std::endl;
					}
				}
			} else {
				server.args.push_back(arg);
			}
		}

		if (MCPConfig::add_server(config_path, server)) {
			std::cout << "Added MCP server '" << server.name << "'" << std::endl;
			return 0;
		}
		return 1;
	}

	if (subcommand == "remove") {
		if (argc < 4) {
			std::cerr << "Usage: shepherd mcp remove <name>" << std::endl;
			return 1;
		}

		std::string name = argv[3];
		if (MCPConfig::remove_server(config_path, name)) {
			std::cout << "Removed MCP server '" << name << "'" << std::endl;
			return 0;
		}
		return 1;
	}

	std::cerr << "Unknown mcp subcommand: " << subcommand << std::endl;
	std::cerr << "Available: add, remove, list" << std::endl;
	return 1;
}

static int handle_edit_system_command(int argc, char** argv) {
	// Load current config
	Config config;
	try {
		config.load();
	} catch (const ConfigError& e) {
		std::cerr << "Error loading config: " << e.what() << std::endl;
		return 1;
	}

	// Create temp file with current system prompt
	std::string temp_path = "/tmp/shepherd_system_prompt_XXXXXX";
	char temp_template[256];
	strncpy(temp_template, temp_path.c_str(), sizeof(temp_template) - 1);
	temp_template[sizeof(temp_template) - 1] = '\0';

	int temp_fd = mkstemp(temp_template);
	if (temp_fd == -1) {
		std::cerr << "Error: Failed to create temporary file" << std::endl;
		return 1;
	}
	temp_path = temp_template;

	// Write current system prompt to temp file
	std::string current_prompt = config.system_prompt;
	if (current_prompt.empty()) {
		// Use default prompt as starting point
		current_prompt = "You have access to tools, but they are OPTIONAL. Only use tools when you need external information that you don't have:\n\n";
		current_prompt += "When you see a NOTICE about conversations moved to long-term memory, you can use the search_memory tool to retrieve that information if the user asks about it.\n\n";
		current_prompt += "Directive: Whenever you invoke a tool, output exactly one line containing only the JSON object for the tool call.\nThe line must start with { and end with }.\nDo not include any text, explanations, extra lines, or formatting before or after the JSON.\nThe JSON must appear on its own line with nothing else.\n\n";
	}

	if (write(temp_fd, current_prompt.c_str(), current_prompt.length()) == -1) {
		std::cerr << "Error: Failed to write to temporary file" << std::endl;
		close(temp_fd);
		unlink(temp_path.c_str());
		return 1;
	}
	close(temp_fd);

	// Get editor from environment, fallback to vi
	const char* editor = getenv("EDITOR");
	if (!editor || editor[0] == '\0') {
		editor = getenv("VISUAL");
	}
	if (!editor || editor[0] == '\0') {
		editor = "vi";
	}

	// Launch editor
	std::string editor_cmd = std::string(editor) + " " + temp_path;
	int result = system(editor_cmd.c_str());
	if (result != 0) {
		std::cerr << "Error: Editor exited with non-zero status" << std::endl;
		unlink(temp_path.c_str());
		return 1;
	}

	// Read edited content
	std::ifstream temp_file(temp_path);
	if (!temp_file.is_open()) {
		std::cerr << "Error: Failed to read edited file" << std::endl;
		unlink(temp_path.c_str());
		return 1;
	}

	std::string new_prompt;
	std::string line;
	while (std::getline(temp_file, line)) {
		if (!new_prompt.empty()) {
			new_prompt += "\n";
		}
		new_prompt += line;
	}
	temp_file.close();

	// Clean up temp file
	unlink(temp_path.c_str());

	// Update config.json
	std::string config_path = Config::get_home_directory() + "/.shepherd/config.json";

	try {
		// Read existing config
		std::ifstream config_file(config_path);
		if (!config_file.is_open()) {
			std::cerr << "Error: Failed to open config file: " << config_path << std::endl;
			return 1;
		}

		nlohmann::json config_json;
		config_file >> config_json;
		config_file.close();

		// Update system prompt
		config_json["system"] = new_prompt;

		// Write back
		std::ofstream out_file(config_path);
		if (!out_file.is_open()) {
			std::cerr << "Error: Failed to write config file: " << config_path << std::endl;
			return 1;
		}

		out_file << config_json.dump(2) << std::endl;
		out_file.close();

		std::cout << "System prompt updated successfully" << std::endl;
		return 0;

	} catch (const std::exception& e) {
		std::cerr << "Error updating config: " << e.what() << std::endl;
		return 1;
	}
}

#if 0
/// @brief Generate JSON schema for a tool
/// @param tool_name Tool name
/// @param description Tool description
/// @param params Parameter definitions
/// @return JSON schema string
std::string generate_tool_json_schema(const std::string& tool_name,
									  const std::string& description,
									  const std::vector<ParameterDef>& params) {
	std::string schema = "{\n";
	schema += "  \"type\": \"function\",\n";
	schema += "  \"function\": {\n";
	schema += "    \"name\": \"" + tool_name + "\",\n";
	schema += "    \"description\": \"" + description + "\",\n";
	schema += "    \"parameters\": {\n";
	schema += "		 \"type\": \"object\",\n";
	schema += "		 \"properties\": {\n";

	// Add properties
	for (size_t i = 0; i < params.size(); ++i) {
		const auto& param = params[i];
		schema += "		   \"" + param.name + "\": {\n";
		schema += "			 \"type\": \"" + param.type + "\"";
		if (!param.description.empty()) {
			schema += ",\n			\"description\": \"" + param.description + "\"";
		}
		schema += "\n		 }";
		if (i < params.size() - 1) {
			schema += ",";
		}
		schema += "\n";
	}

	schema += "		 }";

	// Add required fields
	std::vector<std::string> required_params;
	for (const auto& param : params) {
		if (param.required) {
			required_params.push_back(param.name);
		}
	}
	if (!required_params.empty()) {
		schema += ",\n		\"required\": [";
		for (size_t i = 0; i < required_params.size(); ++i) {
			schema += "\"" + required_params[i] + "\"";
			if (i < required_params.size() - 1) {
				schema += ", ";
			}
		}
		schema += "]";
	}

	schema += "\n	 }\n";
	schema += "  }\n";
	schema += "}";

	return schema;
}

/// @brief Generate JSON schemas section for Llama 3.x first user message
/// @param tool_descriptions Map of tool names to descriptions
/// @param registry Tool registry for accessing tool objects
/// @return Formatted JSON schemas string to prepend to first user message
std::string generate_llama3_tool_schemas(const std::map<std::string, std::string>& tool_descriptions,
										 ToolRegistry& registry) {
	std::string result;

	// Modified BFCL format for multi-turn agent (original BFCL is single-turn only)
	// Make it VERY clear that tools are optional - only use when actually needed
	result += "The following functions are available IF needed to answer the user's request:\n\n";
	result += "IMPORTANT: Only call a function if you actually need external information or capabilities. ";
	result += "For greetings, casual conversation, or questions you can answer directly - respond normally without calling any function.\n\n";
	result += "When you DO need to call a function, respond with ONLY a JSON object in this format: ";
	result += "{\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n\n";

	// Sort tools so memory tools appear first
	std::vector<std::string> memory_tools = {"search_memory", "get_fact", "set_fact", "clear_fact"};
	std::vector<std::string> other_tools;

	// Separate memory tools from other tools
	for (const auto& pair : tool_descriptions) {
		if (std::find(memory_tools.begin(), memory_tools.end(), pair.first) == memory_tools.end()) {
			other_tools.push_back(pair.first);
		}
	}

	// Generate JSON schemas for each tool
	// Memory tools first
	for (const auto& tool_name : memory_tools) {
		if (tool_descriptions.find(tool_name) != tool_descriptions.end()) {
			Tool* tool = registry.get_tool(tool_name);
			if (tool) {
				auto params_schema = tool->get_parameters_schema();
				if (!params_schema.empty()) {
					result += generate_tool_json_schema(tool_name, tool_descriptions.at(tool_name), params_schema);
					result += "\n\n";
				}
			}
		}
	}

	// Other tools
	for (const auto& tool_name : other_tools) {
		Tool* tool = registry.get_tool(tool_name);
		if (tool) {
			auto params_schema = tool->get_parameters_schema();
			if (!params_schema.empty()) {
				result += generate_tool_json_schema(tool_name, tool_descriptions.at(tool_name), params_schema);
				result += "\n\n";
			}
		}
	}

	return result;
}

/// @brief Format tools list based on model family
/// @param config Model configuration
/// @param tool_descriptions Map of tool names to descriptions
/// @param registry Tool registry for accessing tool objects
/// @return Formatted tools string for system prompt
std::string format_tools_for_model(const ModelConfig& config,
								   const std::map<std::string, std::string>& tool_descriptions,
								   ToolRegistry& registry) {
	std::string result;

	// Sort tools so memory tools appear first
	std::vector<std::string> memory_tools = {"search_memory", "get_fact", "set_fact", "clear_fact"};
	std::vector<std::string> other_tools;

	// Separate memory tools from other tools
	for (const auto& pair : tool_descriptions) {
		if (std::find(memory_tools.begin(), memory_tools.end(), pair.first) == memory_tools.end()) {
			other_tools.push_back(pair.first);
		}
	}

	// Llama 3.x uses JSON-based zero-shot tool calling (per PDF page 10-11)
	// Per the spec, JSON schemas should go in the FIRST USER MESSAGE, not system message
	// System message should be minimal - just context about handling tool responses
	// Note: The chat template will add "Environment: ipython" automatically when builtin_tools is set
	if (config.family == ModelFamily::LLAMA_3_X) {
		// For Llama 3.x, return empty string - system message already set above
		// JSON schemas are added to first user message, not system message
		return "";
	}

	// Generic/fallback format (for other models)
	result += "Here are the available tools:  \n\n";

	// Add memory tools first
	for (const auto& tool_name : memory_tools) {
		if (tool_descriptions.find(tool_name) != tool_descriptions.end()) {
			Tool* tool = registry.get_tool(tool_name);
			if (tool) {
				result += "- " + tool_name + ": " + tool_descriptions.at(tool_name) + " (parameters: " + tool->parameters() + ")\n";
			}
		}
	}

	// Add other tools
	for (const auto& tool_name : other_tools) {
		Tool* tool = registry.get_tool(tool_name);
		if (tool) {
			result += "- " + tool_name + ": " + tool_descriptions.at(tool_name) + " (parameters: " + tool->parameters() + ")\n";
		}
	}

	return result;
}
#endif

int main(int argc, char** argv) {
	// Store original arguments for potential MPI re-exec
	set_global_args(argc, argv);

	// Detect MPI rank early (for multi-GPU TensorRT models)
	int mpi_rank = 0;
	const char* mpi_rank_env = getenv("OMPI_COMM_WORLD_RANK");
	if (mpi_rank_env) {
		mpi_rank = std::atoi(mpi_rank_env);
	}
	bool is_mpi_leader = (mpi_rank == 0);

	// Handle edit-system subcommand
	if (argc >= 2 && std::string(argv[1]) == "edit-system") {
		return handle_edit_system_command(argc, argv);
	}

	// Handle list-tools subcommand
	if (argc >= 2 && std::string(argv[1]) == "list-tools") {
		// Initialize tools system
		register_filesystem_tools();
		register_command_tools();
		register_json_tools();
		register_http_tools();
		register_memory_tools();
		register_mcp_resource_tools();
		register_core_tools();

		auto& registry = ToolRegistry::instance();
		auto tool_descriptions = registry.list_tools_with_descriptions();

		printf("\n=== Available Tools (%zu) ===\n\n", tool_descriptions.size());
		for (const auto& pair : tool_descriptions) {
			Tool* tool = registry.get_tool(pair.first);
			if (tool) {
				printf("• %s\n", pair.first.c_str());
				printf("  %s\n", pair.second.c_str());

				// Show parameters if available
				auto params = tool->get_parameters_schema();
				if (!params.empty()) {
					printf("  Parameters:\n");
					for (const auto& param : params) {
						printf("	- %s (%s)%s: %s\n",
							   param.name.c_str(),
							   param.type.c_str(),
							   param.required ? ", required" : "",
							   param.description.c_str());
					}
				} else {
					printf("  Parameters: %s\n", tool->parameters().c_str());
				}
				printf("\n");
			}
		}
		return 0;
	}

	// Handle MCP subcommand
	if (argc >= 2 && std::string(argv[1]) == "mcp") {
		return handle_mcp_command(argc, argv);
	}

	bool debug_override = false;
	bool no_mcp = false;
	int truncate_limit = 0;
	std::string log_file;
	std::string config_file_path;
	std::string model_override;
	std::string model_path_override;
	std::string backend_override;
	std::string api_key_override;
	std::string api_base_override;
	std::string template_override;
	std::string memory_db_override;
	int server_port = 8000;
	std::string server_host = "0.0.0.0";
	int context_size_override = -1;  // -1 means not specified, 0 means use model's full context

	static struct option long_options[] = {
		{"config", required_argument, 0, 'c'},
		{"debug", optional_argument, 0, 'd'},
		{"log-file", required_argument, 0, 'l'},
		{"model", required_argument, 0, 'm'},
		{"model_path", required_argument, 0, 1018},
		{"backend", required_argument, 0, 1002},
		{"api-key", required_argument, 0, 1003},
		{"api-base", required_argument, 0, 1004},
		{"context-size", required_argument, 0, 1000},
		{"memory-db", required_argument, 0, 1023},
		{"nomcp", no_argument, 0, 1005},
		{"template", required_argument, 0, 1006},
		{"server", no_argument, 0, 1015},
		{"port", required_argument, 0, 1016},
		{"host", required_argument, 0, 1017},
		{"truncate", required_argument, 0, 1019},
		{"help", no_argument, 0, 'h'},
		{0, 0, 0, 0}
	};

	int opt;
	int option_index = 0;
	while ((opt = getopt_long(argc, argv, "c:dl:m:h", long_options, &option_index)) != -1) {
		switch (opt) {
			case 'c':
				config_file_path = optarg;
				break;
			case 'd':
				debug_override = true;
				// Parse optional debug level (default to 1 if not specified)
				if (optarg) {
					g_debug_level = atoi(optarg);
				} else {
					g_debug_level = 1;
				}
				break;
			case 'l':
				log_file = optarg;
				break;
			case 'm':
				model_override = optarg;
				break;
			case 1018: // --model_path
				model_path_override = optarg;
				break;
			case 1002: // --backend
				backend_override = optarg;
				break;
			case 1003: // --api-key
				api_key_override = optarg;
				break;
			case 1004: // --api-base
				api_base_override = optarg;
				break;
			case 1000: // --context-size
				context_size_override = std::atoi(optarg);
				if (context_size_override < 0) {
					printf("Error: context-size cannot be negative (use 0 for model's full context)\n");
					return 1;
				}
				break;
			case 1005: // --nomcp
				no_mcp = true;
				break;
			case 1006: // --template
				template_override = optarg;
				break;
			case 1015: // --server
				g_server_mode = true;
				break;
			case 1016: // --port
				server_port = std::atoi(optarg);
				if (server_port <= 0 || server_port > 65535) {
					printf("Error: port must be between 1 and 65535\n");
					return 1;
				}
				break;
			case 1017: // --host
				server_host = optarg;
				break;
			case 1019: // --truncate
				truncate_limit = std::atoi(optarg);
				break;
			case 1023: // --memory-db
				memory_db_override = optarg;
				break;
			case 'h':
				print_usage(argc, argv);
				return 0;
			default:
				print_usage(argc, argv);
				return 1;
		}
	}

	// Check if input is from a terminal or piped (do this early)
	bool is_interactive = isatty(STDIN_FILENO);

	// Initialize logger early
	Logger& logger = Logger::instance();
	// g_debug_level already set during argument parsing, don't overwrite it

	if (g_debug_level) {
		logger.set_log_level(LogLevel::DEBUG);
		std::cout << "Debug mode enabled (level " << g_debug_level << ")" << std::endl;
	} else {
		// Suppress INFO logs unless in debug mode
		logger.set_log_level(LogLevel::WARN);
	}

	// Load configuration
	config = std::make_unique<Config>();

	// Set custom config path if specified
	if (!config_file_path.empty()) {
		config->set_config_path(config_file_path);
	}

	try {
		config->load();
	} catch (const ConfigError& e) {
		fprintf(stderr, "Configuration error: %s\n", e.what());
		return 1;
	}

	// Apply command-line overrides
	if (!backend_override.empty()) {
		config->set_backend(backend_override);
	}
	if (!api_key_override.empty()) {
		config->key = api_key_override;
	}
	if (!api_base_override.empty()) {
		config->api_base = api_base_override;
	}
	if (context_size_override >= 0) {  // -1 means not set, 0+ are valid values
		config->context_size = context_size_override;
	}
	if (truncate_limit > 0) {
		config->truncate_limit = truncate_limit;
	}

	// Validate configuration (skip model path check if overridden)
	if (model_override.empty() && model_path_override.empty()) {
		// No overrides - validate everything from config
		try {
			config->validate();
		} catch (const ConfigError& e) {
			fprintf(stderr, "Configuration error: %s\n", e.what());
			fprintf(stderr, "Edit ~/.shepherd/config->json or use -h for help\n");
			return 1;
		}
	} else {
		// With overrides, just validate backend availability
		auto available = Config::get_available_backends();
		bool backend_found = false;
		for (const auto& b : available) {
			if (b == config->backend) {
				backend_found = true;
				break;
			}
		}
		if (!backend_found) {
			std::string available_str;
			for (size_t i = 0; i < available.size(); ++i) {
				if (i > 0) available_str += ", ";
				available_str += available[i];
			}
			fprintf(stderr, "Invalid backend '%s'. Available: %s\n",
					config->backend.c_str(), available_str.c_str());
			return 1;
		}
		// Validate model file exists (only for local backends with overrides)
		if (config->backend == "llamacpp" || config->backend == "tensorrt") {
			// Construct the full path to validate
			std::string model_file_to_check;
			std::string model_name = model_override.empty() ? config->model : model_override;
			std::string model_dir = model_path_override.empty() ? config->model_path : model_path_override;

			// Check if model_name is already a full path
			if (!model_name.empty() && (model_name[0] == '/' || model_name[0] == '~')) {
				model_file_to_check = model_name;
			} else {
				model_file_to_check = std::filesystem::path(model_dir) / model_name;
			}

			// Expand ~ if present
			if (!model_file_to_check.empty() && model_file_to_check[0] == '~') {
				model_file_to_check = Config::get_home_directory() + model_file_to_check.substr(1);
			}

			if (!std::filesystem::exists(model_file_to_check)) {
				fprintf(stderr, "Model file not found: %s\n", model_file_to_check.c_str());
				return 1;
			}
		}
	}

	if (!log_file.empty()) {
		logger.set_log_file(log_file);
		logger.set_console_output(false);
		LOG_INFO("Logging to file: " + log_file);
	}

	LOG_INFO("Shepherd starting up...");
	LOG_INFO("Backend: " + config->backend);

	// Set up signal handlers for graceful shutdown
	signal(SIGINT, signal_handler);
	signal(SIGTERM, signal_handler);
	signal(SIGUSR1, cancel_handler);  // For cancellation from FastAPI on client disconnect

	size_t context_size = context_size_override >= 0 ? static_cast<size_t>(context_size_override) : config->context_size;

	// Create and initialize backend (needed for both CLI and server mode)
	try {
		// Create backend manager
		std::string model_path;

		// Apply command-line overrides to config (for local backends)
		if (config->backend == "llamacpp" || config->backend == "tensorrt") {
			if (!model_override.empty()) {
				config->model = model_override;
				LOG_INFO("Model override from command line: " + model_override);
			}
			if (!model_path_override.empty()) {
				config->model_path = model_path_override;
				LOG_INFO("Model path override from command line: " + model_path_override);
			}
		}

		// Show status to user (only for local backends that actually load models)
		bool is_local_backend = (config->backend == "llamacpp" || config->backend == "tensorrt");
		if (is_interactive && is_local_backend) {
			printf("Initializing Engine...\n");
			fflush(stdout);
		}

		// Create the backend
		auto backend = BackendFactory::create_backend(config->backend, context_size);

		if (!backend) {
			LOG_ERROR("Failed to create backend: " + config->backend);
			return 1;
		}


		// Initialize RAG system with specified or default path (skip in server mode - clients have their own RAG)
		if (!g_server_mode) {
			std::string db_path;
			if (!memory_db_override.empty()) {
				// Command-line override takes precedence
				db_path = memory_db_override;
				// Expand ~ if present
				if (db_path[0] == '~') {
					db_path = Config::get_home_directory() + db_path.substr(1);
				}
				LOG_INFO("Using memory database from command line: " + db_path);
			} else if (!config->memory_database.empty()) {
				// Config file value
				db_path = config->memory_database;
				// Expand ~ if present
				if (db_path[0] == '~') {
					db_path = Config::get_home_directory() + db_path.substr(1);
				}
				LOG_INFO("Using memory database from config: " + db_path);
			} else {
				// Default
				try {
					db_path = Config::get_home_directory() + "/.shepherd/memory.db";
				} catch (const ConfigError& e) {
					LOG_ERROR("Failed to determine home directory: " + std::string(e.what()));
					return 1;
				}
			}
			if (!RAGManager::initialize(db_path, config->max_db_size)) {
				LOG_ERROR("Failed to initialize RAG system");
				return 1;
			}
		}

		// Show status to user before the slow model load (only for local backends)
		if (is_interactive && is_local_backend) {
			printf("Loading Model...\n");
			fflush(stdout);
		}

		// Create the session that will be used throughout (not in server mode)
		Session session;
		session.system_message = config->system_message;

		// Enable auto-eviction for API backends when context size is specified
		// Local backends (llamacpp) handle eviction through reactive callbacks
		session.auto_evict = (context_size > 0 && !backend->is_local);

		// Initialize tools system (skip in server mode - tools handled by client)
		if (!g_server_mode) {
			LOG_INFO("Initializing tools system...");
			try {
				// Register all native tools including memory search
				register_filesystem_tools();
				register_command_tools();
				register_json_tools();
				register_http_tools();
				register_memory_tools();
				register_mcp_resource_tools();
				register_core_tools();	// IMPORTANT: Register core tools (Bash, Glob, Grep, Edit, WebSearch, etc.)

				auto& registry = ToolRegistry::instance();
				auto tools = registry.list_tools();
				LOG_INFO("Native tools initialized with " + std::to_string(tools.size()) + " tools");
				if (g_debug_level) {
					for (const auto& tool_name : tools) {
						LOG_DEBUG("Registered tool: " + tool_name);
					}
				}

				// Initialize MCP servers (will register additional tools)
				if (!no_mcp) {
					auto& mcp = MCP::instance();
					mcp.initialize();

					// Show total tool count after MCP
					tools = registry.list_tools();
					LOG_INFO("Total tools available: " + std::to_string(tools.size()) +
							 " (native + " + std::to_string(mcp.get_tool_count()) + " MCP)");
				} else {
					LOG_INFO("MCP system disabled via --nomcp flag");
					tools = registry.list_tools();
					LOG_INFO("Total tools available: " + std::to_string(tools.size()) + " (native only)");
				}

			} catch (const std::exception& e) {
				LOG_ERROR("Failed to initialize tools system: " + std::string(e.what()));
				return 1;
			}
		} else {
			LOG_INFO("Server mode: Tool registration skipped (client-side tools)");
		}

		// Get registry and tool descriptions (needed for both interactive and server modes)
		auto& registry = ToolRegistry::instance();
		auto tool_descriptions = registry.list_tools_with_descriptions();

		LOG_INFO("Shepherd initialization complete");

		// ============================================================================
		// Server Mode - HTTP API via FastAPI
		// ============================================================================
		if (g_server_mode) {
			// Initialize backend before starting server
			LOG_DEBUG("Initializing backend for server mode...");
			backend->initialize(session);
			return run_server(backend, server_host, server_port);
		}

		// For MPI multi-GPU setups, only rank 0 handles user interaction
		// Non-zero ranks keep their backend alive for MPI communication
		LOG_DEBUG("MPI rank check: mpi_rank=" + std::to_string(mpi_rank) + ", is_mpi_leader=" + std::string(is_mpi_leader ? "true" : "false"));
		if (!is_mpi_leader) {
			LOG_INFO("MPI rank " + std::to_string(mpi_rank) + " initialization complete, waiting for work from rank 0...");
			// Keep process alive - backend will be controlled via MPI by rank 0
			while (true) {
				std::this_thread::sleep_for(std::chrono::seconds(3600));
			}
		}

		// From here on, only rank 0 continues...
		LOG_DEBUG("Rank 0 continuing to user input loop...");

		// Force interactive mode for rank 0 (mpirun can make isatty return false)
		if (is_mpi_leader) {
			is_interactive = true;
		}

		// Populate tools from registry
		// This converts from ToolRegistry format to Session::Tool format
		for (const auto& [tool_name, tool_desc] : tool_descriptions) {
			Session::Tool st;
			st.name = tool_name;
			st.description = tool_desc;

			// Get parameters schema from the actual tool
			Tool* tool = registry.get_tool(tool_name);
			if (tool) {
				auto params_schema = tool->get_parameters_schema();
				if (!params_schema.empty()) {
					// Build proper JSON schema from ParameterDef vector
					nlohmann::json params_json;
					params_json["type"] = "object";
					params_json["properties"] = nlohmann::json::object();

					std::vector<std::string> required_fields;
					for (const auto& param : params_schema) {
						nlohmann::json prop;
						prop["type"] = param.type;
						if (!param.description.empty()) {
							prop["description"] = param.description;
						}
						if (!param.default_value.empty()) {
							prop["default"] = param.default_value;
						}
						params_json["properties"][param.name] = prop;

						if (param.required) {
							required_fields.push_back(param.name);
						}
					}

					if (!required_fields.empty()) {
						params_json["required"] = required_fields;
					}

					st.parameters = params_json;
				} else {
					// Legacy tools without structured schema - create minimal schema
					st.parameters = nlohmann::json::object();
					st.parameters["type"] = "object";
					st.parameters["properties"] = nlohmann::json::object();
				}
			}

			session.tools.push_back(st);
		}

		LOG_DEBUG("Session created with " + std::to_string(session.tools.size()) + " tools");

		// Initialize backend (calibrate tokens, validate setup, etc.)
		LOG_DEBUG("Initializing backend...");
		backend->initialize(session);
		session.backend = backend.get();

		// ============================================================================

	// ============================================================================
	// Server Mode - HTTP API via FastAPI
	// ============================================================================
	if (g_server_mode) {
		return run_server(backend, server_host, server_port);
	}

	// ============================================================================
	// CLI Mode - Interactive or Piped Input
	// ============================================================================
	return run_cli(backend, session);

} catch (const std::exception& e) {
	fprintf(stderr, "Fatal error: %s\n", e.what());
	LOG_FATAL("Fatal error: " + std::string(e.what()));
	return 1;
}
}

