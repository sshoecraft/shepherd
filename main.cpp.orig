#include "logger.h"
#include "config.h"
#include "backend_manager.h"
#include "rag.h"
#include "tools/tools.h"
#include "tools/tool_parser.h"
#include "mcp/mcp_manager.h"
#include "mcp/mcp_config.h"
#include "nlohmann/json.hpp"

#ifdef ENABLE_API_BACKENDS
#include "backends/openai.h"
#include "backends/ollama.h"
#endif

#include <iostream>
#include <fstream>
#include <string>
#include <cstring>
#include <getopt.h>
#include <csignal>
#include <memory>
#include <unistd.h>
#include <filesystem>
#include <termios.h>
#include <sys/select.h>
#include <fcntl.h>
#include <atomic>

#ifdef USE_READLINE
#include <readline/readline.h>
#include <readline/history.h>
#endif

// Global debug flag
bool g_debug_mode = false;

// Get input line with readline support if available
// Global flag to track EOF from readline
bool g_eof_received = false;

// Global cancellation flag
std::atomic<bool> g_generation_cancelled{false};

// Terminal state management
struct termios g_original_term;
bool g_term_raw_mode = false;

// Set terminal to raw mode for immediate key detection
void set_terminal_raw() {
    if (g_term_raw_mode) return;

    struct termios raw;
    tcgetattr(STDIN_FILENO, &g_original_term);
    raw = g_original_term;

    // Disable canonical mode and echo
    raw.c_lflag &= ~(ICANON | ECHO);
    raw.c_cc[VMIN] = 0;   // Non-blocking read
    raw.c_cc[VTIME] = 0;

    tcsetattr(STDIN_FILENO, TCSANOW, &raw);
    g_term_raw_mode = true;
}

// Restore terminal to normal mode
void restore_terminal() {
    if (!g_term_raw_mode) return;
    tcsetattr(STDIN_FILENO, TCSANOW, &g_original_term);
    g_term_raw_mode = false;
}

// Check if Escape key was pressed (non-blocking)
bool check_escape_pressed() {
    if (!g_term_raw_mode) return false;

    fd_set readfds;
    FD_ZERO(&readfds);
    FD_SET(STDIN_FILENO, &readfds);

    struct timeval tv = {0, 0};  // No wait

    if (select(STDIN_FILENO + 1, &readfds, nullptr, nullptr, &tv) > 0) {
        char c;
        if (read(STDIN_FILENO, &c, 1) == 1) {
            if (c == 27) {  // ESC key
                return true;
            }
        }
    }

    return false;
}

// Helper to clean bracketed paste sequences from a string
static std::string strip_bracketed_paste_sequences(const std::string& input) {
    std::string result = input;

    // Strip common bracketed paste markers that leak through EditLine
    // Variants seen: [200~, 200~, 00~ (start) and [201~, 201~, 01~ (end)

    // Remove paste start sequences (various formats)
    std::vector<std::string> start_markers = {"[200~", "200~", "00~"};
    for (const auto& marker : start_markers) {
        size_t pos = 0;
        while ((pos = result.find(marker, pos)) != std::string::npos) {
            result.erase(pos, marker.length());
        }
    }

    // Remove paste end sequences (various formats)
    std::vector<std::string> end_markers = {"[201~", "201~", "01~"};
    for (const auto& marker : end_markers) {
        size_t pos = 0;
        while ((pos = result.find(marker, pos)) != std::string::npos) {
            result.erase(pos, marker.length());
        }
    }

    return result;
}

std::string get_input_line(const char* prompt, bool is_interactive) {
#ifdef USE_READLINE
    if (is_interactive) {
        char* line = readline(prompt);
        if (line == nullptr) {
            g_eof_received = true;
            return "";  // EOF
        }
        g_eof_received = false;
        std::string result(line);

        // Debug: Show what we actually received
        if (g_debug_mode) {
            LOG_DEBUG("Raw input received: '" + result + "'");
            // Show hex bytes for debugging escape sequences
            std::string hex_dump;
            for (unsigned char c : result.substr(0, std::min(size_t(50), result.length()))) {
                char buf[10];
                snprintf(buf, sizeof(buf), "%02x ", c);
                hex_dump += buf;
            }
            LOG_DEBUG("First 50 bytes (hex): " + hex_dump);
        }

        // Detect bracketed paste mode (EditLine on macOS doesn't strip these automatically)
        // If we see the paste start sequence, accumulate until paste end
        // Check for various formats: 200~, 00~, [200~, etc.
        if (result.find("200~") != std::string::npos || result.find("00~") != std::string::npos) {
            LOG_DEBUG("Detected bracketed paste start");
            std::string pasted_content = strip_bracketed_paste_sequences(result);

            // Keep reading lines until we see the paste end marker (various formats)
            while (result.find("201~") == std::string::npos && result.find("01~") == std::string::npos) {
                free(line);
                line = readline("");  // No prompt for continuation
                if (line == nullptr) {
                    g_eof_received = true;
                    break;
                }
                result = std::string(line);

                // Check for end marker (various formats)
                if (result.find("201~") != std::string::npos || result.find("01~") != std::string::npos) {
                    // Add final content before end marker
                    std::string final_line = strip_bracketed_paste_sequences(result);
                    if (!final_line.empty()) {
                        pasted_content += "\n" + final_line;
                    }
                    break;
                } else {
                    // Accumulate this line
                    pasted_content += "\n" + result;
                }
            }

            free(line);
            if (!pasted_content.empty()) {
                add_history(pasted_content.c_str());
            }
            LOG_DEBUG("Bracketed paste complete, length: " + std::to_string(pasted_content.length()));
            return pasted_content;
        }

        // Check for triple-quote multi-line mode (alternative for explicit multi-line)
        if (result == "\"\"\"" || result.find("\"\"\"") == 0) {
            // Multi-line mode - keep reading until closing """
            std::string multi_line;
            bool first_line = true;

            // If line has content after """, include it
            if (result.length() > 3) {
                multi_line = result.substr(3);
                first_line = false;
            }

            while (true) {
                free(line);  // Free previous line
                line = readline("");  // No prompt for continuation lines
                if (line == nullptr) {
                    g_eof_received = true;
                    break;
                }

                std::string continued(line);

                // Check for closing """
                if (continued == "\"\"\"" || continued.find("\"\"\"") != std::string::npos) {
                    // If """ is at end of line with content before it, include that content
                    size_t pos = continued.find("\"\"\"");
                    if (pos > 0) {
                        if (!first_line) multi_line += "\n";
                        multi_line += continued.substr(0, pos);
                    }
                    break;
                }

                // Add line to multi-line buffer
                if (!first_line) multi_line += "\n";
                multi_line += continued;
                first_line = false;
            }

            if (!multi_line.empty()) {
                add_history(multi_line.c_str());
            }
            free(line);
            return multi_line;
        }

        if (!result.empty()) {
            add_history(line);
        }
        free(line);
        return result;
    } else {
        // Non-interactive mode - use getline
        std::string line;
        if (!std::getline(std::cin, line)) {
            g_eof_received = true;
            return "";  // EOF
        }
        g_eof_received = false;
        return line;
    }
#else
    // No readline - use basic getline
    if (is_interactive) {
        std::cout << prompt << std::flush;
    }
    std::string line;
    if (!std::getline(std::cin, line)) {
        g_eof_received = true;
        return "";  // EOF
    }
    g_eof_received = false;
    return line;
#endif
}

static void print_usage(int, char** argv) {
    printf("\n=== Shepherd - Advanced LLM Management System ===\n");
    printf("\nUsage:\n");
    printf("    %s [OPTIONS]\n", argv[0]);
    printf("    %s edit-system              Edit system prompt in $EDITOR\n", argv[0]);
    printf("    %s mcp <add|remove|list> [args...]\n", argv[0]);
    printf("\nOptions:\n");
    printf("    -c, --config       Show current configuration\n");
    printf("    -d, --debug        Enable debug mode\n");
    printf("    -l, --log-file     Log to file instead of console\n");
    printf("    -m, --model        Model path (overrides config)\n");
    printf("    --backend          Backend (llamacpp, openai, anthropic, gemini, grok, ollama)\n");
    printf("    --api-key          API key for cloud backends\n");
    printf("    --api-base         API base URL (for OpenAI-compatible APIs)\n");
    printf("    --context-size     Set context window size (default: from config)\n");
    printf("    --max-tokens       Set max generation tokens (default: auto)\n");
    printf("    --nomcp            Disable MCP system (no MCP servers loaded)\n");
    printf("    --template         Custom chat template file (Jinja format, llamacpp only)\n");
    printf("    -h, --help         Show this help message\n");
    printf("\nMCP Management:\n");
    printf("    mcp list                              List all configured MCP servers\n");
    printf("    mcp add <name> <cmd> [args] [-e ...]  Add a new MCP server\n");
    printf("                                          -e KEY=VALUE  Set environment variable\n");
    printf("    mcp remove <name>                     Remove an MCP server\n");
    printf("\nConfiguration:\n");
    printf("    Edit ~/.shepherd/config.json to configure:\n");
    printf("    - backend: llamacpp, openai, anthropic");
#ifdef PLATFORM_LINUX
#ifdef ENABLE_TENSORRT
    printf(", tensorrt");
#endif
#endif
    printf("\n");
    printf("    - model: model name or path\n");
    printf("    - model_path: directory for models (optional, defaults to ~/.shepherd/models)\n");
    printf("    - key: API key for cloud backends (optional)\n");
    printf("    - context_size: context window size (optional, 0 = auto)\n");
    printf("\nFeatures:\n");
    printf("    - Direct document reading and processing\n");
    printf("    - Conversation memory with search capabilities\n");
    printf("    - Multiple inference backends (local + cloud)\n");
    printf("    - Tool execution support\n");
    printf("    - Model Context Protocol (MCP) server integration\n");
    printf("\n");
}

static void signal_handler(int signal) {
    restore_terminal();
    printf("\n\nReceived signal %d, shutting down gracefully...\n", signal);
    exit(0);
}

static void show_config(const Config& config) {
    printf("\n=== Current Configuration ===\n");
    printf("Backend: %s\n", config.get_backend().c_str());
    printf("Model: %s\n", config.get_model().c_str());

    if (config.get_backend() == "llamacpp" || config.get_backend() == "tensorrt") {
        printf("Model path: %s\n", config.get_model_path().c_str());
    } else if (config.get_backend() == "openai" || config.get_backend() == "anthropic") {
        printf("API key: %s\n", config.get_key().empty() ? "(not set)" : "***");
    }

    printf("Context size: %zu\n", config.get_context_size());

    printf("\nAvailable backends: ");
    auto available = Config::get_available_backends();
    for (size_t i = 0; i < available.size(); ++i) {
        if (i > 0) printf(", ");
        printf("%s", available[i].c_str());
    }
    printf("\n\n");
}

static int handle_mcp_command(int argc, char** argv) {
    if (argc < 3) {
        std::cerr << "Usage: shepherd mcp <add|remove|list> [args...]" << std::endl;
        return 1;
    }

    std::string subcommand = argv[2];
    std::string config_path = std::string(getenv("HOME")) + "/.shepherd/config.json";

    if (subcommand == "list") {
        // Suppress logs during health check
        Logger::instance().set_log_level(LogLevel::FATAL);
        MCPConfig::list_servers(config_path, true);  // Check health
        return 0;
    }

    if (subcommand == "add") {
        if (argc < 5) {
            std::cerr << "Usage: shepherd mcp add <name> <command> [args...] [-e KEY=VALUE ...]" << std::endl;
            return 1;
        }

        MCPServerEntry server;
        server.name = argv[3];
        server.command = argv[4];

        // Parse arguments and environment variables
        for (int i = 5; i < argc; i++) {
            std::string arg = argv[i];

            if (arg == "-e" || arg == "--env") {
                // Next arg should be KEY=VALUE
                if (i + 1 < argc) {
                    i++;
                    std::string env_pair = argv[i];
                    size_t eq_pos = env_pair.find('=');
                    if (eq_pos != std::string::npos) {
                        std::string key = env_pair.substr(0, eq_pos);
                        std::string value = env_pair.substr(eq_pos + 1);
                        server.env[key] = value;
                    } else {
                        std::cerr << "Warning: Invalid env format (use KEY=VALUE): " << env_pair << std::endl;
                    }
                }
            } else {
                server.args.push_back(arg);
            }
        }

        if (MCPConfig::add_server(config_path, server)) {
            std::cout << "Added MCP server '" << server.name << "'" << std::endl;
            return 0;
        }
        return 1;
    }

    if (subcommand == "remove") {
        if (argc < 4) {
            std::cerr << "Usage: shepherd mcp remove <name>" << std::endl;
            return 1;
        }

        std::string name = argv[3];
        if (MCPConfig::remove_server(config_path, name)) {
            std::cout << "Removed MCP server '" << name << "'" << std::endl;
            return 0;
        }
        return 1;
    }

    std::cerr << "Unknown mcp subcommand: " << subcommand << std::endl;
    std::cerr << "Available: add, remove, list" << std::endl;
    return 1;
}

static int handle_edit_system_command(int argc, char** argv) {
    // Load current config
    Config config;
    try {
        config.load();
    } catch (const ConfigError& e) {
        std::cerr << "Error loading config: " << e.what() << std::endl;
        return 1;
    }

    // Create temp file with current system prompt
    std::string temp_path = "/tmp/shepherd_system_prompt_XXXXXX";
    char temp_template[256];
    strncpy(temp_template, temp_path.c_str(), sizeof(temp_template) - 1);
    temp_template[sizeof(temp_template) - 1] = '\0';

    int temp_fd = mkstemp(temp_template);
    if (temp_fd == -1) {
        std::cerr << "Error: Failed to create temporary file" << std::endl;
        return 1;
    }
    temp_path = temp_template;

    // Write current system prompt to temp file
    std::string current_prompt = config.get_system_prompt();
    if (current_prompt.empty()) {
        // Use default prompt as starting point
        current_prompt = "You have access to tools, but they are OPTIONAL. Only use tools when you need external information that you don't have:\n\n";
        current_prompt += "When you see a NOTICE about conversations moved to long-term memory, you can use the search_memory tool to retrieve that information if the user asks about it.\n\n";
        current_prompt += "When calling a tool, output ONLY the JSON tool call - no explanations, no preamble, no markdown formatting. Just the raw JSON object starting with { and ending with }.";
    }

    if (write(temp_fd, current_prompt.c_str(), current_prompt.length()) == -1) {
        std::cerr << "Error: Failed to write to temporary file" << std::endl;
        close(temp_fd);
        unlink(temp_path.c_str());
        return 1;
    }
    close(temp_fd);

    // Get editor from environment, fallback to vi
    const char* editor = getenv("EDITOR");
    if (!editor || editor[0] == '\0') {
        editor = getenv("VISUAL");
    }
    if (!editor || editor[0] == '\0') {
        editor = "vi";
    }

    // Launch editor
    std::string editor_cmd = std::string(editor) + " " + temp_path;
    int result = system(editor_cmd.c_str());
    if (result != 0) {
        std::cerr << "Error: Editor exited with non-zero status" << std::endl;
        unlink(temp_path.c_str());
        return 1;
    }

    // Read edited content
    std::ifstream temp_file(temp_path);
    if (!temp_file.is_open()) {
        std::cerr << "Error: Failed to read edited file" << std::endl;
        unlink(temp_path.c_str());
        return 1;
    }

    std::string new_prompt;
    std::string line;
    while (std::getline(temp_file, line)) {
        if (!new_prompt.empty()) {
            new_prompt += "\n";
        }
        new_prompt += line;
    }
    temp_file.close();

    // Clean up temp file
    unlink(temp_path.c_str());

    // Update config.json
    std::string config_path = Config::get_home_directory() + "/.shepherd/config.json";

    try {
        // Read existing config
        std::ifstream config_file(config_path);
        if (!config_file.is_open()) {
            std::cerr << "Error: Failed to open config file: " << config_path << std::endl;
            return 1;
        }

        nlohmann::json config_json;
        config_file >> config_json;
        config_file.close();

        // Update system prompt
        config_json["system"] = new_prompt;

        // Write back
        std::ofstream out_file(config_path);
        if (!out_file.is_open()) {
            std::cerr << "Error: Failed to write config file: " << config_path << std::endl;
            return 1;
        }

        out_file << config_json.dump(2) << std::endl;
        out_file.close();

        std::cout << "System prompt updated successfully" << std::endl;
        return 0;

    } catch (const std::exception& e) {
        std::cerr << "Error updating config: " << e.what() << std::endl;
        return 1;
    }
}

int main(int argc, char** argv) {
    // Handle edit-system subcommand
    if (argc >= 2 && std::string(argv[1]) == "edit-system") {
        return handle_edit_system_command(argc, argv);
    }

    // Handle MCP subcommand
    if (argc >= 2 && std::string(argv[1]) == "mcp") {
        return handle_mcp_command(argc, argv);
    }

    bool show_config_only = false;
    bool debug_override = false;
    bool no_mcp = false;
    std::string log_file;
    std::string model_path_override;
    std::string backend_override;
    std::string api_key_override;
    std::string api_base_override;
    int context_size_override = 0;
    int max_tokens_override = 0;

    static struct option long_options[] = {
        {"config", no_argument, 0, 'c'},
        {"debug", no_argument, 0, 'd'},
        {"log-file", required_argument, 0, 'l'},
        {"model", required_argument, 0, 'm'},
        {"backend", required_argument, 0, 1002},
        {"api-key", required_argument, 0, 1003},
        {"api-base", required_argument, 0, 1004},
        {"context-size", required_argument, 0, 1000},
        {"max-tokens", required_argument, 0, 1001},
        {"nomcp", no_argument, 0, 1005},
        {"template", required_argument, 0, 1006},
        {"help", no_argument, 0, 'h'},
        {0, 0, 0, 0}
    };

    int opt;
    int option_index = 0;
    while ((opt = getopt_long(argc, argv, "cdl:m:h", long_options, &option_index)) != -1) {
        switch (opt) {
            case 'c':
                show_config_only = true;
                break;
            case 'd':
                debug_override = true;
                break;
            case 'l':
                log_file = optarg;
                break;
            case 'm':
                model_path_override = optarg;
                break;
            case 1002: // --backend
                backend_override = optarg;
                break;
            case 1003: // --api-key
                api_key_override = optarg;
                break;
            case 1004: // --api-base
                api_base_override = optarg;
                break;
            case 1000: // --context-size
                context_size_override = std::atoi(optarg);
                if (context_size_override <= 0) {
                    printf("Error: context-size must be positive\n");
                    return 1;
                }
                break;
            case 1001: // --max-tokens
                max_tokens_override = std::atoi(optarg);
                if (max_tokens_override <= 0) {
                    printf("Error: max-tokens must be positive\n");
                    return 1;
                }
                break;
            case 1005: // --nomcp
                no_mcp = true;
                break;
            case 'h':
                print_usage(argc, argv);
                return 0;
            default:
                print_usage(argc, argv);
                return 1;
        }
    }

    // Check if input is from a terminal or piped (do this early)
    bool is_interactive = isatty(STDIN_FILENO);

    // Initialize logger early
    Logger& logger = Logger::instance();
    g_debug_mode = debug_override;

    if (g_debug_mode) {
        logger.set_log_level(LogLevel::DEBUG);
        std::cout << "Debug mode enabled" << std::endl;
    } else {
        // Suppress INFO logs unless in debug mode
        logger.set_log_level(LogLevel::WARN);
    }

    // Load configuration
    Config config;
    try {
        config.load();
    } catch (const ConfigError& e) {
        fprintf(stderr, "Configuration error: %s\n", e.what());
        return 1;
    }

    // Apply command-line overrides
    if (!backend_override.empty()) {
        config.set_backend(backend_override);
    }
    if (!api_key_override.empty()) {
        config.set_key(api_key_override);
    }
    if (!api_base_override.empty()) {
        config.set_api_base(api_base_override);
    }
    if (context_size_override > 0) {
        config.set_context_size(context_size_override);
    }

    // Show config if requested
    if (show_config_only) {
        show_config(config);
        return 0;
    }

    // Validate configuration (skip model path check if overridden)
    if (model_path_override.empty()) {
        try {
            config.validate();
        } catch (const ConfigError& e) {
            fprintf(stderr, "Configuration error: %s\n", e.what());
            fprintf(stderr, "Edit ~/.shepherd/config.json or use -h for help\n");
            return 1;
        }
    } else {
        // With model override, just validate backend availability
        auto available = Config::get_available_backends();
        bool backend_found = false;
        for (const auto& b : available) {
            if (b == config.get_backend()) {
                backend_found = true;
                break;
            }
        }
        if (!backend_found) {
            std::string available_str;
            for (size_t i = 0; i < available.size(); ++i) {
                if (i > 0) available_str += ", ";
                available_str += available[i];
            }
            fprintf(stderr, "Invalid backend '%s'. Available: %s\n",
                    config.get_backend().c_str(), available_str.c_str());
            return 1;
        }
        // Validate model file exists (only for local backends)
        if (config.get_backend() == "llamacpp" || config.get_backend() == "tensorrt") {
            if (!std::filesystem::exists(model_path_override)) {
                fprintf(stderr, "Model file not found: %s\n", model_path_override.c_str());
                return 1;
            }
        }
    }

    if (!log_file.empty()) {
        logger.set_log_file(log_file);
        logger.set_console_output(false);
        LOG_INFO("Logging to file: " + log_file);
    }

    LOG_INFO("Shepherd starting up...");
    LOG_INFO("Backend: " + config.get_backend());

    // Set up signal handlers for graceful shutdown
    signal(SIGINT, signal_handler);
    signal(SIGTERM, signal_handler);

    try {
        // Create backend manager
        std::string model_path;

        if (!model_path_override.empty()) {
            // Use command line override as full path
            model_path = model_path_override;
            LOG_INFO("Model path overridden from command line: " + model_path);
        } else if (config.get_backend() == "llamacpp" || config.get_backend() == "tensorrt") {
            // For local backends, construct full path from directory + model name
            std::filesystem::path full_path;
            if (!config.get_model().empty() && (config.get_model()[0] == '/' || config.get_model()[0] == '~')) {
                // Model is already a full path
                full_path = config.get_model();
            } else {
                // Combine model_path directory with model name
                full_path = std::filesystem::path(config.get_model_path()) / config.get_model();
            }
            model_path = full_path.string();
        }

        size_t context_size = context_size_override > 0 ?
                              static_cast<size_t>(context_size_override) :
                              config.get_context_size();

        if (context_size_override > 0) {
            LOG_INFO("Context size overridden from command line: " + std::to_string(context_size));
            config.set_context_size(context_size);
        }

        if (max_tokens_override > 0) {
            LOG_INFO("Max tokens overridden from command line: " + std::to_string(max_tokens_override));
        }

        // Show status to user
        if (is_interactive) {
            printf("Initializing Engine...\n");
            fflush(stdout);
        }

        std::unique_ptr<BackendManager> backend = BackendFactory::create_backend(
            config.get_backend(),
            model_path,
            context_size,
            "" // api_key will be set during initialization
        );

        if (!backend) {
            LOG_ERROR("Failed to create backend: " + config.get_backend());
            return 1;
        }

        // Initialize RAG system with default path
        std::string db_path;
        try {
            db_path = Config::get_home_directory() + "/.shepherd/memory.db";
        } catch (const ConfigError& e) {
            LOG_ERROR("Failed to determine home directory: " + std::string(e.what()));
            return 1;
        }
        if (!RAGManager::initialize(db_path)) {
            LOG_ERROR("Failed to initialize RAG system");
            return 1;
        }

        // Show status to user before the slow model load
        if (is_interactive) {
            printf("Loading Model...\n");
            fflush(stdout);
        }

        // Initialize backend with appropriate credentials
        bool init_success = false;
        if (config.get_backend() == "llamacpp" || config.get_backend() == "tensorrt") {
            init_success = backend->initialize(model_path, "");
        } else if (config.get_backend() == "openai" || config.get_backend() == "anthropic") {
            // Set API base if specified (for OpenAI-compatible APIs)
            if (!config.get_api_base().empty() && config.get_backend() == "openai") {
                // Cast to OpenAIBackend to call set_api_base
                #ifdef ENABLE_API_BACKENDS
                auto* openai_backend = dynamic_cast<OpenAIBackend*>(backend.get());
                if (openai_backend) {
                    openai_backend->set_api_base(config.get_api_base());
                }
                #endif
            }
            // Use model override if provided, otherwise use config
            std::string api_model_name = model_path_override.empty() ? config.get_model() : model_path_override;
            init_success = backend->initialize(api_model_name, config.get_key());
        } else if (config.get_backend() == "ollama") {
            // Set API base if specified
            if (!config.get_api_base().empty()) {
                #ifdef ENABLE_API_BACKENDS
                auto* ollama_backend = dynamic_cast<OllamaBackend*>(backend.get());
                if (ollama_backend) {
                    ollama_backend->set_api_base(config.get_api_base());
                }
                #endif
            }
            // Use model override if provided, otherwise use config
            std::string api_model_name = model_path_override.empty() ? config.get_model() : model_path_override;
            // Ollama doesn't require a real API key
            std::string api_key = config.get_key().empty() ? "dummy" : config.get_key();
            init_success = backend->initialize(api_model_name, api_key);
        } else {
            init_success = backend->initialize("", "");
        }

        if (!init_success) {
            LOG_ERROR("Failed to initialize backend: " + config.get_backend());
            return 1;
        }

        LOG_INFO("Backend initialized: " + backend->get_backend_name() + " with model: " + backend->get_model_name());

        // Initialize tools system
        LOG_INFO("Initializing tools system...");
        try {
            // Register all native tools including memory search
            // DISABLED: Only memory tool is active
            // register_filesystem_tools();
            // register_command_tools();
            // register_json_tools();
            // register_http_tools();
            register_memory_tools();
            // register_mcp_resource_tools();

            auto& registry = ToolRegistry::instance();
            auto tools = registry.list_tools();
            LOG_INFO("Native tools initialized with " + std::to_string(tools.size()) + " tools");
            if (g_debug_mode) {
                for (const auto& tool_name : tools) {
                    LOG_DEBUG("Registered tool: " + tool_name);
                }
            }

            // Initialize MCP servers (will register additional tools)
            if (!no_mcp) {
                auto& mcp_manager = MCPManager::instance();
                mcp_manager.initialize(config);

                // Show total tool count after MCP
                tools = registry.list_tools();
                LOG_INFO("Total tools available: " + std::to_string(tools.size()) +
                         " (native + " + std::to_string(mcp_manager.get_tool_count()) + " MCP)");
            } else {
                LOG_INFO("MCP system disabled via --nomcp flag");
                tools = registry.list_tools();
                LOG_INFO("Total tools available: " + std::to_string(tools.size()) + " (native only)");
            }

        } catch (const std::exception& e) {
            LOG_ERROR("Failed to initialize tools system: " + std::string(e.what()));
            return 1;
        }

        // Check what tools the model already knows from chat template
        if (g_debug_mode) {
            LOG_DEBUG("Checking model's built-in tool support...");
        }

        // Build complete system message:
        // 1. Custom system prompt from config (if exists)
        // 2. Standard system context with tool behavior rules
        // 3. Available tools list (memory tools first)

        std::string system_message;

        // Part 0: Critical format instruction FIRST (models pay most attention to early content)
        system_message = "IMPORTANT: Tool calls MUST be the entire message with EXACTLY this JSON format:\n";
        system_message += "{\"name\": \"function_name_here\", \"parameters\": {\"key\": \"value\"}}\n";
        system_message += "Use field \"name\" (not \"tool_name\") and \"parameters\" (not \"params\").\n\n";

        // Part 1: Custom system prompt from config (if exists)
        std::string custom_system = config.get_system_prompt();
        if (!custom_system.empty()) {
            system_message += custom_system + "\n\n";
        }

        // Part 2: Standard system context (as specified by user)
        system_message += "You have access to tools, most of which are OPTIONAL.  \n\n";
        system_message += "**Memory & RAG Usage:**  \n";
        system_message += "Memory tools (`search_memory`, `get_fact`, `set_fact`, `clear_fact`) are core tools. Use them **whenever you need past conversation details, stored facts, or context that is not in the current conversation**, or when a system NOTICE indicates that context has been moved to long-term memory. This includes:  \n";
        system_message += "- When the user asks about information that is no longer visible.  \n";
        system_message += "- When you need prior conversation context to answer correctly.  \n";
        system_message += "- When a NOTICE indicates that context has been stored in memory.  \n\n";
        system_message += "Do not use memory tools if the required information is already present in the current conversation. Use them judiciously, only when context or facts are missing.  \n\n";
        system_message += "**Tool Call Rules:**  \n";
        system_message += "- Tool calls must be raw JSON only (no code fences, no quotes, no text).  \n";
        system_message += "- A tool call must be the entire message, beginning with `{` and ending with `}`.  \n";
        system_message += "- If a tool fails or is unavailable, continue gracefully without retrying indefinitely.  \n\n";
        system_message += "**Answering Rules:**  \n";
        system_message += "- Tool results contain authoritative external data. Trust them and use them directly in your answers.  \n";
        system_message += "- Do not reinterpret or modify tool outputs. Use them as-is.  \n";
        system_message += "- When no tool is needed, answer normally as an AI assistant.  \n\n";
        system_message += "Here are the available tools:  \n\n";

        auto& registry = ToolRegistry::instance();
        auto tool_descriptions = registry.list_tools_with_descriptions();

        // Sort tools so memory tools appear first
        std::vector<std::string> memory_tools = {"search_memory", "get_fact", "set_fact", "clear_fact"};
        std::vector<std::string> other_tools;

        // Separate memory tools from other tools
        for (const auto& pair : tool_descriptions) {
            if (std::find(memory_tools.begin(), memory_tools.end(), pair.first) == memory_tools.end()) {
                other_tools.push_back(pair.first);
            }
        }

        // Add memory tools first
        for (const auto& tool_name : memory_tools) {
            if (tool_descriptions.find(tool_name) != tool_descriptions.end()) {
                Tool* tool = registry.get_tool(tool_name);
                if (tool) {
                    system_message += "- " + tool_name + ": " + tool_descriptions[tool_name] + " (parameters: " + tool->parameters() + ")\n";
                }
            }
        }

        // Add other tools
        for (const auto& tool_name : other_tools) {
            Tool* tool = registry.get_tool(tool_name);
            if (tool) {
                system_message += "- " + tool_name + ": " + tool_descriptions[tool_name] + " (parameters: " + tool->parameters() + ")\n";
            }
        }

        backend->add_system_message(system_message);
        LOG_INFO("Added system prompt with " + std::to_string(tool_descriptions.size()) + " available tools");

        LOG_INFO("Shepherd initialization complete");

        // Warm up models that need loading (llamacpp, ollama)
        // This pre-loads the model into memory to avoid delay on first user prompt
        // Cloud APIs (OpenAI, Anthropic, Gemini, Grok) don't need warmup
        std::string backend_name = backend->get_backend_name();
        if (backend_name == "llamacpp" || backend_name == "ollama") {
            try {
                LOG_DEBUG("Warming up " + backend_name + " model...");
                backend->add_user_message("Ready.");
                std::string warmup_response = backend->generate(50); // Short generation

                // Clear warmup from context - we only wanted to load the model
                backend->get_context_manager().clear();

                // Re-add system prompt after clearing warmup
                backend->add_system_message(system_message);

                LOG_DEBUG("Model warmup complete");
            } catch (const std::exception& e) {
                LOG_WARN("Model warmup failed: " + std::string(e.what()));
            }
        }

        std::string user_input;
        while (true) {
            user_input = get_input_line(is_interactive ? "\033[32m> \033[0m" : "", is_interactive);

            if (user_input.empty()) {
                // Empty string indicates EOF (Ctrl+D) or empty input
                if (g_eof_received || std::cin.eof()) {
                    if (!is_interactive) {
                        LOG_DEBUG("End of piped input");
                    } else {
                        LOG_INFO("User pressed Ctrl+D - exiting");
                    }
                    break;
                } else if (is_interactive) {
                    // Interactive mode: empty line continues (just prompt again)
                    continue;
                } else {
                    // In non-interactive mode, skip empty lines
                    continue;
                }
            }

            // Check for exit commands
            if (user_input == "exit" || user_input == "quit") {
                LOG_INFO("User requested exit with command: " + user_input);
                break;
            }

            LOG_DEBUG("User input: " + user_input);

            // Show user message in transcript (only in non-interactive mode)
            if (!is_interactive) {
                printf("> %s\n", user_input.c_str());
                fflush(stdout);
            }

            try {
                // Add user message to backend context
                LOG_DEBUG("Adding user message to backend");
                backend->add_user_message(user_input);

                // Reset cancellation flag
                g_generation_cancelled = false;

                // Enable raw terminal mode for escape detection during generation
                if (is_interactive) {
                    set_terminal_raw();
                }

                // Tool execution loop - orchestrated by main
                // No iteration limit - user can cancel with Ctrl+C or ESC if needed
                std::string response;
                int tool_loop_iteration = 0;

                while (true) {
                    // Check for cancellation between iterations
                    if (g_generation_cancelled || (is_interactive && check_escape_pressed())) {
                        g_generation_cancelled = true;
                        LOG_DEBUG("Generation cancelled by user");
                        if (is_interactive) {
                            restore_terminal();
                            printf("\n\033[31m[Cancelled]\033[0m\n");
                        }
                        break;
                    }

                    tool_loop_iteration++;
                    LOG_DEBUG("Tool loop iteration: " + std::to_string(tool_loop_iteration));

                    // Generate response from backend
                    LOG_DEBUG("Calling backend->generate()");
                    response = backend->generate(max_tokens_override);
                    LOG_DEBUG("backend->generate() returned, length: " + std::to_string(response.length()));

                    // Check if user cancelled during generation
                    if (is_interactive && check_escape_pressed()) {
                        g_generation_cancelled = true;
                        restore_terminal();
                        printf("\n\033[31m[Cancelled]\033[0m\n");
                        break;
                    }

                    // Parse for tool calls
                    auto tool_call_opt = ToolParser::parse_tool_call(response, {});

                    if (tool_call_opt.has_value()) {
                        // Tool call detected
                        auto tool_call = tool_call_opt.value();
                        std::string tool_name = tool_call.name;
                        std::string tool_call_id = tool_call.tool_call_id;
                        std::string json_str = tool_call.raw_json;

                        LOG_DEBUG("Tool call detected: " + tool_name + (tool_call_id.empty() ? "" : " (id: " + tool_call_id + ")"));

                        // Show tool call in transcript with actual parameter values
                        std::string params_str;
                        bool first_param = true;
                        for (const auto& param : tool_call.parameters) {
                            if (!first_param) params_str += ", ";
                            first_param = false;

                            // Convert std::any value to string for display
                            std::string value_str;
                            try {
                                if (param.second.type() == typeid(std::string)) {
                                    value_str = std::any_cast<std::string>(param.second);
                                } else if (param.second.type() == typeid(int)) {
                                    value_str = std::to_string(std::any_cast<int>(param.second));
                                } else if (param.second.type() == typeid(double)) {
                                    value_str = std::to_string(std::any_cast<double>(param.second));
                                } else if (param.second.type() == typeid(bool)) {
                                    value_str = std::any_cast<bool>(param.second) ? "true" : "false";
                                } else {
                                    value_str = "<unknown>";
                                }
                            } catch (...) {
                                value_str = "<error>";
                            }

                            // Truncate long values
                            if (value_str.length() > 50) {
                                value_str = value_str.substr(0, 47) + "...";
                            }

                            params_str += param.first + "=" + value_str;
                        }
                        if (is_interactive) {
                            printf("\033[33m< %s(%s)\033[0m\n", tool_name.c_str(), params_str.c_str());
                        } else {
                            printf("< %s(%s)\n", tool_name.c_str(), params_str.c_str());
                        }
                        fflush(stdout);

                        // Get tool from registry
                        auto& registry = ToolRegistry::instance();
                        Tool* tool = registry.get_tool(tool_name);
                        if (!tool) {
                            LOG_ERROR("Tool not found: " + tool_name);
                            response = "Error: Tool '" + tool_name + "' not found";
                            break;
                        }

                        // Execute tool
                        LOG_DEBUG("Executing tool: " + tool_name);
                        auto result = tool->execute(tool_call.parameters);

                        // Extract tool result content
                        std::string tool_result;
                        auto success_it = result.find("success");
                        if (success_it != result.end() && std::any_cast<bool>(success_it->second)) {
                            auto content_it = result.find("content");
                            if (content_it != result.end()) {
                                tool_result = std::any_cast<std::string>(content_it->second);
                            } else {
                                tool_result = "Tool executed successfully";
                            }
                        } else {
                            auto error_it = result.find("error");
                            if (error_it != result.end()) {
                                tool_result = "Error: " + std::any_cast<std::string>(error_it->second);
                            } else {
                                tool_result = "Error: Tool execution failed";
                            }
                        }

                        std::string result_preview = tool_result.substr(0, std::min(size_t(80), tool_result.length()));
                        if (tool_result.length() > 80) result_preview += "...";
                        LOG_DEBUG("Tool result: " + result_preview);

                        // Show tool result in transcript (truncated)
                        std::string truncated_result = tool_result;
                        size_t first_newline = truncated_result.find('\n');
                        if (first_newline != std::string::npos) {
                            size_t second_newline = truncated_result.find('\n', first_newline + 1);
                            if (second_newline != std::string::npos) {
                                truncated_result = truncated_result.substr(0, second_newline) + "\n...";
                            }
                        } else if (truncated_result.length() > 100) {
                            truncated_result = truncated_result.substr(0, 100) + "...";
                        }
                        if (is_interactive) {
                            printf("\033[36m> %s\033[0m\n", truncated_result.c_str());
                        } else {
                            printf("> %s\n", truncated_result.c_str());
                        }
                        fflush(stdout);

                        // Add assistant message with tool call JSON
                        backend->add_assistant_message(json_str);

                        // Add tool result to context (with tool_call_id if available)
                        backend->add_tool_result(tool_name, tool_result, tool_call_id);

                        // Continue loop to get model's response to tool result
                        continue;
                    } else {
                        // No tool call - this is the final response
                        LOG_DEBUG("No tool call detected, final response");

                        // Show assistant response in transcript
                        if (!response.empty()) {
                            if (is_interactive) {
                                printf("\033[33m< %s\033[0m\n", response.c_str());
                            } else {
                                printf("< %s\n", response.c_str());
                            }
                            fflush(stdout);
                        }

                        backend->add_assistant_message(response);
                        break;
                    }
                }

                LOG_DEBUG("Response from backend (first 100 chars): '" + response.substr(0, std::min(size_t(100), response.length())) + "'");
                LOG_DEBUG("Response from backend length: " + std::to_string(response.length()));

                // Note: Response printing is now handled inline in the tool loop above
                // with transcript-style formatting (< for assistant, > for user/tool results)

                // Restore terminal to normal mode after generation
                if (is_interactive) {
                    restore_terminal();
                }

            } catch (const std::exception& e) {
                // Restore terminal on error
                if (is_interactive) {
                    restore_terminal();
                    printf("\033[31mError: %s\033[0m\n", e.what());
                } else {
                    fprintf(stderr, "Error: %s\n", e.what());
                }
                LOG_ERROR("Error processing turn: " + std::string(e.what()));
            }
        }

    } catch (const std::exception& e) {
        restore_terminal();
        printf("\033[31mFatal error: %s\033[0m\n", e.what());
        LOG_FATAL("Fatal error: " + std::string(e.what()));
        return 1;
    }

    restore_terminal();
    LOG_INFO("Shutting down Shepherd...");
    LOG_INFO("Shutdown complete");

    return 0;
}